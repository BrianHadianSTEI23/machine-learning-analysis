# Entropy
Entropy is a measure that represents how likely a state could be. Entropy utilizes stochastic principles such that
for a given state, there will always be a value that represents how the equation of the problem is applied 
into the state. That value is called Surprise (s) and with a given equation of the problem, probability of getting
a certain 'outcome' (outcome based on that equation to solve) will be related to probability (by measuring the likeliness
of getting that outcome divided by all outcome possible). Surprise (s) can be stated as below
$$ s = log (1 / probability) $$
because suprise is related to the inverse of probability and the log will make the function reach 0 and 1 more easily 
and accurately.  

By calculating the suprise (s) for each distinct outcome, entropy for that particular state follow formula below
$$ Entropy = \sigma(x * p(x)) $$
with x represents the surprise of certain outcome of current state and p(x) represents the probability of that certain
outcome of current state.  
  
For each entropy calculated from each state, the highest value of entropy represents that the state have the least amount
of randomness (almost equal probability to get every outcome from that state), while the lowest value of entropy 
represents the highest amount of randomness (high difference of probability for every outcome from that state)

# Motivation
Given a population and best algorithm possible, how do you quantify the randomness that will affect the algorithm result?

# Limitation
Only works for categorization task

# Algorithm
1. (Do later)

# Example
(Do later)
